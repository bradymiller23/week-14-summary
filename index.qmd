---
title: "Weekly Summary Template"
author: "Brady Miller"
title-block-banner: true
title-block-style: default
toc: true
# format: html
format: pdf
---

```{r}
packages <- c(
    # Old packages
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "repr",
    "tidyverse",
    "kableExtra",
    "IRdisplay",
    # NEW
    "torch",
    "torchvision",
    "luz",
    "dimRed",
    "RSpectra",
    'corrplot',
    'car'
)

renv::install(packages)
sapply(packages, require, character.only=TRUE)
```

## Tuesday, April 18

::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Finding PCAs
1. Factor loadings and their meaning
1. Item 3
:::

#### Finding PCAs
```{r}
data <- tibble(
  x1 = rnorm(100, mean = 0, sd = 1),
  x2 = x1 + rnorm(100, mean = 0, sd = 0.1),
  x3 = x1 + rnorm(100, mean = 0, sd = 0.1)
)

head(data) %>% knitr::kable()
```

```{r}
pca <- princomp(data, cor = TRUE)
summary(pca)

par(mfrow=c(1, 2))

Z_pca <- predict(pca, data)
plot(data)
```

```{r}
plot(Z_pca)
```


#### Finding factor loadings and their meanings
```{r}
pca$loadings
```


#### Example of finding PCAs using test score data

```{r}
set.seed(42)

n <- 500
science <- rnorm(n, mean = 60, sd = 10)
humanities <- rnorm(n, mean = 80, sd=10)

df <- tibble(
  math = 0.8 * science + rnorm(n, mean = 0, sd = 7),
  physics = 1.0 * science + rnorm(n, mean = 0, sd = 5),
  chemistry = 1.3 * science + rnorm(n, mean = 0, sd = 3),
  history = 0.8 * humanities + rnorm(n, mean = 0, sd = 5),
  geography = 1.0 * humanities + rnorm(n, mean = 0, sd = 10),
  literature = 1.2 * humanities + rnorm(n, mean = 0, sd = 2)
)

df %>%
    head() %>%
    round(digits = 2) %>%
    knitr::kable()
```


```{r}
plot(df$math, df$physics)
```

```{r}
pca <- princomp(df, cor=TRUE)
summary(pca)
```


```{r}
plot(pca, type="l")
```



## Thursday, April 20



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Principal component regression using test score data
1. Nonlinear dimension reduction
1. Autoencoder/decoder process
:::

#### Principal component regression using test score data
Principal component regression
```{r}
df$gpa <- (0.9 * science + 0.5 * humanities + rnorm(n, mean=0, sd=10)) * 4 / 100

df %>% 
    head() %>%
    round(digits=2) %>%
    knitr::kable()
```


```{r}
lm_fit <- lm(gpa ~ ., df)
summary(lm_fit)
```


```{r}
df %>% 
    cor() %>% 
    corrplot(diag=F)


pca <- princomp(df %>% select(-gpa), cor=TRUE)
screeplot(pca)
```

```{r}
Z <- predict(pca, df)
```

```{r}
df_pca <- Z %>% 
    as_tibble %>% 
    select(Comp.1, Comp.2) %>% 
    mutate(gpa = df$gpa)

head(df_pca) %>% knitr::kable()
```

```{r}
df_pca %>% 
    cor() %>% 
    corrplot(diag=F)

lm_pca <- lm(gpa ~ ., df_pca)
summary(lm_pca)
```

```{r}
vif(lm_pca) %>% t
```

#### Nonlinear dimension reduction
```{r}
generate_two_spirals <- function(){
  set.seed(42)
  n <- 500
  noise <- 0.05
  t <- (1:n) / n * 4 * pi
  x1 <- t * (sin(t) + rnorm(n, 0, noise))
  x2 <- t * (cos(t) + rnorm(n, 0, noise))
  y  <- t
  return(tibble(x1=x1, x2=x2, y=y))
}

df <- generate_two_spirals()
head(df)

ggplot(df) +
    geom_point(aes(x=x1, y=x2, col=y), alpha=0.5) +
    scale_colour_gradient(low="red", high="blue")
```


```{r}
pca <- princomp(df[, 1:2], cor=T)
pca$loadings
```

```{r}
df_pca <- predict(pca, df)
head(df_pca)

ggplot(as_tibble(df_pca)) +
    geom_point(aes(x=Comp.1, y=0, col=df$y), alpha=0.5) +
    scale_colour_gradient(low="red", high="blue")
```


#### Autoencoders

```{r}
autoencoder <- nn_module(
    initialize = function(p, q1, q2, q3, o) {
    self$encoder <- nn_sequential(
        nn_linear(p, q1), nn_relu(),
        nn_linear(q1, q2), nn_relu(),
        nn_linear(q2, q3), nn_relu(),
        nn_linear(q3, o)
    )
    self$decoder <- nn_sequential(
        nn_linear(o, q3), nn_relu(),
        nn_linear(q3, q2), nn_relu(),
        nn_linear(q2, q1), nn_relu(),
        nn_linear(q1, p)
    )
    },
    forward = function(x) {
    x %>%
        torch_reshape(c(-1, 28 * 28)) %>% 
        self$encoder() %>%
        self$decoder() %>% 
        torch_reshape(c(-1, 28, 28))
    },
    predict = function(x) {
    x %>% 
        torch_reshape(c(-1, 28 * 28)) %>% 
        self$encoder()     
    }
)

dir <- "./mnist"

train_ds <- mnist_dataset(
    root = dir,
    train = TRUE,
    download = TRUE,
    transform = transform
)
test_ds <- mnist_dataset(
    root = dir,
    train = FALSE,
    download = TRUE,
    transform = transform
)

X <- test_ds
inputs <- torch_tensor(X$data * 1.0)

plot_image = \(x) image(t(x)[1:28, 28:1], useRaster=TRUE, axes=FALSE, col=gray.colors(1000))
```

Original vs. Decoded (at initialization)
```{r}
AE <- autoencoder(p = 28 * 28, q1 = 32, q2 = 16, q3 = 8, o = 2)

par(mfrow=c(4, 2))

set.seed(123)
for(k in 1:4){
    i <- sample(1:10000, 1)
    input <- inputs[i]
    output <- AE(inputs[i:i])[1]

    par(mfrow=c(1, 2))
    plot_image(inputs[i] %>% as_array)
    title("Original")
    
    plot_image(output %>% as_array)
    title("Decoded")
}
```


Fitting the autoencoder using luz
```{r}
ae_fit <- autoencoder %>%
    setup(
        loss = nn_mse_loss(),
        optimizer = optim_adam
    ) %>%

    set_hparams(
        p=28*28, q1=128, q2=64, q3=32, o=2
    ) %>%
    
    set_opt_hparams(
        lr=1e-3
    ) %>%

    fit(
        data = list(
            inputs, 
            inputs # targets are the same as inputs
        ),
        epochs=30,
        verbose=TRUE,
        dataloader_options = list(
            batch_size = 100, 
            shuffle=TRUE
        ),
        callbacks = list(
            luz_callback_lr_scheduler(
                torch::lr_step, 
                step_size = 10, 
                gamma=1.01
                )
        )
    )


plot(ae_fit)
```


 Lower-dimensional Encoding of the Data
```{r}
X_dim2 <- predict(ae_fit, inputs) %>% as_array()
head(X_dim2)

df_ae <- tibble(
    x1 = X_dim2[, 1],
    x2 = X_dim2[, 2],
    y = as.factor(X$targets - 1)
)

ggplot(df_ae) +
    geom_point(aes(x=x1, y=x2, col=y))
```


Original vs. Decoded (after fitting)
```{r}
par(mfrow=c(4, 2))

set.seed(123)
for(k in 1:4){
    i <- sample(1:10000, 1)
    input <- inputs[i]
    output <- ae_fit$model$forward(inputs[i:i])[1]

    par(mfrow=c(1, 2))
    plot_image(inputs[i] %>% as_array)
    title("Original")
    
    plot_image(output %>% as_array)
    title("Decoded")
}
```